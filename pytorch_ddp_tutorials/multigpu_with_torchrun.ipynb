{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datautils import MyTrainDataset\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        rank: Unique identifier of each process\n",
    "        world_size: Total number of processes\n",
    "    \"\"\"\n",
    "    # os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    # os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    # torch.cuda.set_device(rank) # we dont need to define thise variables as torch run will take care for us\n",
    "    init_process_group(backend=\"nccl\") # we just need assign what backend we are using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_data: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        gpu_id: int,\n",
    "        save_every: int,\n",
    "        snapshot_path: str,\n",
    "\n",
    "    ) -> None:\n",
    "        # self.gpu_id = \n",
    "        # gpu_id we dont need to provide this explicitly\n",
    "        self.gpu_id=int(os.environ[\"LOCAL_RANK\"])\n",
    "        self.model = model.to(self.gpu_id)\n",
    "        self.train_data = train_data\n",
    "        self.optimizer = optimizer\n",
    "        self.save_every = save_every\n",
    "        self.epochs_run=0\n",
    "        if os.path.exists(snapshot_path):\n",
    "            print(\"Loading snapshot\")\n",
    "            self._load_snapshot(snapshot_path)\n",
    "        self.model = DDP(self.model, device_ids=[self.gpu_id])\n",
    "    \n",
    "    def _load_snapshot(self,snapshot_path):\n",
    "        snapshot=torch.load(snapshot_path)\n",
    "        self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n",
    "        self.epochs_run=snapshot[\"EPOCHS_RUN\"]\n",
    "\n",
    "\n",
    "    def _run_batch(self, source, targets):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(source)\n",
    "        loss = F.cross_entropy(output, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _run_epoch(self, epoch):\n",
    "        b_sz = len(next(iter(self.train_data))[0])\n",
    "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
    "        self.train_data.sampler.set_epoch(epoch)\n",
    "        for source, targets in self.train_data:\n",
    "            source = source.to(self.gpu_id)\n",
    "            targets = targets.to(self.gpu_id)\n",
    "            self._run_batch(source, targets)\n",
    "\n",
    "    def _save_snapshot(self, epoch):\n",
    "        snapshot={}\n",
    "        snapshot[\"MODEL_STATE\"] = self.model.module.state_dict()\n",
    "        snapshot[\"EPOCHS_RUN\"]=epoch\n",
    "        PATH = \"snapshot.pt\"\n",
    "        torch.save(snapshot, PATH)\n",
    "        print(f\"Epoch {epoch} | Training snapshot saved at {PATH}\")\n",
    "\n",
    "    def train(self, max_epochs: int):\n",
    "        for epoch in range(self.epochs_run,max_epochs):\n",
    "            self._run_epoch(epoch)\n",
    "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
    "                self._save_snapshot(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_objs():\n",
    "    train_set = MyTrainDataset(2048)  # load your dataset\n",
    "    model = torch.nn.Linear(20, 1)  # load your model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    return train_set, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        sampler=DistributedSampler(dataset)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main( save_every: int, total_epochs: int, batch_size: int,snapshot_path: str=\"snapshot.pt\"):\n",
    "    ddp_setup()\n",
    "    dataset, model, optimizer = load_train_objs()\n",
    "    train_data = prepare_dataloader(dataset, batch_size)\n",
    "    trainer = Trainer(model, train_data, optimizer, save_every,snapshot_path)\n",
    "    trainer.train(total_epochs)\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    total_epochs=int(sys.argv[1])\n",
    "    save_every=int(sys.argv[2])\n",
    "    main(save_every,total_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
